<div align='center'>
  <h3>
    Q-learning üÜöÔ∏è Sarsa Algorithm 
  </h3>
</div>


O objetivo do projeto √© a compara√ß√£o entre o desempenho de dois algor√≠timos muito utilizados em Reinforcement Learning, o [QLearning](https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-q-learning) e o [Sarsa](https://towardsdatascience.com/reinforcement-learning-with-sarsa-a-good-alternative-to-q-learning-algorithm-bf35b209e1c). Para demonstrar os diferentes desempenhos, utilizou-se dois ambientes implementados pela biblioteca `gym`, o [TaxiDriver](https://www.gymlibrary.dev/environments/toy_text/taxi/) e o [Cliff Walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/)

### Configura√ß√µes ‚öôÔ∏è

Instale as bibliotecas necess√°rias utilizando o comando:

```bash

pip install -r requirements.txt

```

Para rodar o projeto siga o padr√£o:

```bash

python main.py <ambiente> <algoritimo> <reuse_data>

```

Onde:

- **ambiente** : 'taxi' ou 'cliff'. 

Ambiente onde o algor√≠timo ocorrer√°.

- **algoritimo**: 'q' ou 'sarsa' ou 'both' . 

Algor√≠timo que ser√° utilizado para resolver o problema. 

Caso a op√ß√£o 'both' seja escolhida, ambos os algor√≠timos rodaram e um gr√°fico de sem desempenho comaprativo ser√° criado, caso utilizado essa op√ß√£o, **reuse_data = 0** necessariamente.

- **reuse_data** : '0' ou '1'. 

Utilizar ou n√£o um csv existente como valor da **q_table**. 

### TaxiDriver - Desempenho üöïÔ∏è

<div align='center'>
  <img src='img/taxi.gif' height='200'/>
</div>

Analisando  o gr√°fico de compara√ß√£o abaixo, que representa o n√∫mero m√©dio de rewards por epis√≥dio de treino, parece que ambos os algor√≠timos (qlearning e sarsa) possiem uma taxa de aprendizagem e comportamento semelhante. Por√©m, analizando a quantidade de a√ß√µes realizadas para chegarmos a solu√ß√£o do problema, √© poss√≠vel chegar a conclus√£o que o **algor√≠timo Sarsa** nem sempre consegue o caminho √≥timo ate o seu destino, tendo que tomar , muitas vezes, um n√∫mero de a√ß√µes maior do que o **algor√≠timo QLearning**.

<div align='center'>
  <img src='results/taxi_comparing_algorithms.jpg' width='300'/>
  <img src='results/qTaxiDriver_actions_per_episode.jpg' width='300'/>
  <img src='results/sarsaTaxiDriver_actions_per_episode.jpg' width='300'/>
</div>


### Cliff Walking - Desempenho üßô‚Äç‚ôÇÔ∏èÔ∏è

<div align='center'>
  <img src='img/cliff_walking.gif' height='150'/>
</div>

Novamente, o comportamento do gr√°fico de compara√ß√£o que leva em conta n√∫mero m√©dio de rewards por epis√≥dio de treino, parece bem semelhante para ambos os algor√≠timos (QLearning e Sarsa). Por√©m, observa-se tamb√©m para esse ambiente que o algor√≠timo Sarsa n√£o realiza o n√∫mero m√≠nimo de steps para realizar o seu objetivo, ou seja, n√£o toma o cam√≠nho √≥timo como solu√ß√£o, diferentemente do QLearning.

<div align='center'>
  <img src='results/cliff_comparing_algorithms.jpg' width='300'/>
</div>

### QLearning vs  Sarsa - Vantagens e Desvantagens üìåÔ∏è 

- `QLearning`

Desenvolvimento da f√≥rmula:
$$Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma max(Q(S_{t+1}, a)) - Q(S_t, A_t) ]$$

- `Sarsa`
Desenvolvimento da f√≥rmula:
$$Q(S_t, A_t) = Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) ]$$

